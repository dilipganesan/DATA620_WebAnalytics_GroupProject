{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5 : Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dilip Ganesan, Raghu Ramnath and  Ahmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data:  http://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "\n",
    "For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.\n",
    "\n",
    "This assignment is due end of day on Monday, 11/12.  You may work in a small team if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Document Classifier Using Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of SPAMDATA SET\n",
    "spamdataset = pd.read_csv(\"https://raw.githubusercontent.com/dilipganesan/DATA620_WebAnalytics_GroupProject/master/Assignment%205/spamfiles/spamdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail     ...       char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000     ...       4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413     ...          0.038575     0.139030   \n",
       "std           0.278616        0.644755     ...          0.243471     0.270355   \n",
       "min           0.000000        0.000000     ...          0.000000     0.000000   \n",
       "25%           0.000000        0.000000     ...          0.000000     0.000000   \n",
       "50%           0.000000        0.000000     ...          0.000000     0.065000   \n",
       "75%           0.000000        0.160000     ...          0.000000     0.188000   \n",
       "max           5.260000       18.180000     ...          4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total         spam  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamdataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...   char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...          0.00        0.000   \n",
       "1             0.00            0.94  ...          0.00        0.132   \n",
       "2             0.64            0.25  ...          0.01        0.143   \n",
       "3             0.31            0.63  ...          0.00        0.137   \n",
       "4             0.31            0.63  ...          0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamdataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_make                float64\n",
       "word_freq_address             float64\n",
       "word_freq_all                 float64\n",
       "word_freq_3d                  float64\n",
       "word_freq_our                 float64\n",
       "word_freq_over                float64\n",
       "word_freq_remove              float64\n",
       "word_freq_internet            float64\n",
       "word_freq_order               float64\n",
       "word_freq_mail                float64\n",
       "word_freq_receive             float64\n",
       "word_freq_will                float64\n",
       "word_freq_people              float64\n",
       "word_freq_report              float64\n",
       "word_freq_addresses           float64\n",
       "word_freq_free                float64\n",
       "word_freq_business            float64\n",
       "word_freq_email               float64\n",
       "word_freq_you                 float64\n",
       "word_freq_credit              float64\n",
       "word_freq_your                float64\n",
       "word_freq_font                float64\n",
       "word_freq_000                 float64\n",
       "word_freq_money               float64\n",
       "word_freq_hp                  float64\n",
       "word_freq_hpl                 float64\n",
       "word_freq_george              float64\n",
       "word_freq_650                 float64\n",
       "word_freq_lab                 float64\n",
       "word_freq_labs                float64\n",
       "word_freq_telnet              float64\n",
       "word_freq_857                 float64\n",
       "word_freq_data                float64\n",
       "word_freq_415                 float64\n",
       "word_freq_85                  float64\n",
       "word_freq_technology          float64\n",
       "word_freq_1999                float64\n",
       "word_freq_parts               float64\n",
       "word_freq_pm                  float64\n",
       "word_freq_direct              float64\n",
       "word_freq_cs                  float64\n",
       "word_freq_meeting             float64\n",
       "word_freq_original            float64\n",
       "word_freq_project             float64\n",
       "word_freq_re                  float64\n",
       "word_freq_edu                 float64\n",
       "word_freq_table               float64\n",
       "word_freq_conference          float64\n",
       "char_freq_;                   float64\n",
       "char_freq_(                   float64\n",
       "char_freq_[                   float64\n",
       "char_freq_!                   float64\n",
       "char_freq_$                   float64\n",
       "char_freq_#                   float64\n",
       "capital_run_length_average    float64\n",
       "capital_run_length_longest      int64\n",
       "capital_run_length_total        int64\n",
       "spam                            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamdataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam is the response variable. We would like to see how many of these are Spam and how many are Ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamcount = len(spamdataset[spamdataset.spam==1])\n",
    "hamcount = len(spamdataset[spamdataset.spam==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will print and see the spam and ham counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813\n",
      "2788\n"
     ]
    }
   ],
   "source": [
    "print(spamcount)\n",
    "print(hamcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that the Spam email are 1813.\n",
    "\n",
    "This is a classification problem. We will be using Random Forest and SVM Classifier to classify spam vs ham.\n",
    "Before starting on the classification, we have to split the dataset in to training datatset and testing dataset.\n",
    "\n",
    "We know the last column spam is the response variable Y and rest of the variables will be split in to X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spamdataset.values[:, 0:57]\n",
    "Y = spamdataset.values[:, 57]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use the train_test_split from sklearn package to split the spamdataset into X_train, X_test, Y_train, Y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 38)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Document Classifier Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42) \n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict for test data sample\n",
    "model_prediction_train = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_output(sample_test, prediction, score=None):\n",
    "    con_mat = metrics.confusion_matrix(sample_test, prediction)\n",
    "    classreport = metrics.classification_report(sample_test, prediction)\n",
    "    try:\n",
    "        print('Random Forrest Classifier Model:')\n",
    "        print(prediction)\n",
    "        print('\\nConfusion Matrix:')\n",
    "        print(con_mat)\n",
    "        print('\\nClassification Rport:')\n",
    "        print(classreport)\n",
    "    except: \n",
    "        raise\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forrest Classifier Model:\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "\n",
      "Confusion Matrix:\n",
      "[[824  31]\n",
      " [ 45 481]]\n",
      "\n",
      "Classification Rport:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.96      0.96       855\n",
      "        1.0       0.94      0.91      0.93       526\n",
      "\n",
      "avg / total       0.94      0.94      0.94      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = model.score(X_test, Y_test)\n",
    "print_model_output(Y_test, model_prediction_train, score=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.944967414916727"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test,model_prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Feature Importance:\n",
      "(0.1357694675873437, array([ 0.   ,  1.08 ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
      "        0.   ,  1.08 ,  0.   ,  3.26 ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
      "        0.   ,  0.   ,  5.43 ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
      "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
      "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
      "        0.   ,  0.   ,  0.   ,  0.   ,  1.08 ,  0.   ,  0.   ,  0.   ,\n",
      "        0.   ,  0.   ,  0.   ,  0.169,  0.   ,  0.   ,  2.052, 21.   ,\n",
      "       39.   ]))\n"
     ]
    }
   ],
   "source": [
    "# Random forest feature importance\n",
    "# display the relative importance of each attribute\n",
    "importances = model.feature_importances_\n",
    "#Sort it\n",
    "print (\"Sorted Feature Importance:\")\n",
    "sorted_feature_importance = sorted(zip(importances, list(X_train)), reverse=True)\n",
    "print(sorted_feature_importance[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier has an accuracy of 94%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Document Classifier Using Support Vector Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Random Forest Classifier, we will use the SVM to see the accuracy and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.SVC(random_state=1)\n",
    "svm.fit(X_train, Y_train)\n",
    "\n",
    "svm_prediction_train = svm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_svm_output(sample_test, prediction, score=None):\n",
    "    con_mat = metrics.confusion_matrix(sample_test, prediction)\n",
    "    classreport = metrics.classification_report(sample_test, prediction)\n",
    "    try:\n",
    "        print('Support Vector Machine Classifier:')\n",
    "        print(prediction)\n",
    "        print('\\nConfusion Matrix:')\n",
    "        print(con_mat)\n",
    "        print('\\nClassification Rport:')\n",
    "        print(classreport)\n",
    "    except: \n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Classifier:\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "\n",
      "Confusion Matrix:\n",
      "[[735 120]\n",
      " [105 421]]\n",
      "\n",
      "Classification Rport:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.86      0.87       855\n",
      "        1.0       0.78      0.80      0.79       526\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmscore = svm.score(X_test, Y_test)\n",
    "print_svm_output(Y_test,svm_prediction_train, score=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83707458363504705"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test,svm_prediction_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Document Classifier Using Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "modelt = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s take a look at our model’s attributes\n",
    "modelt.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9051412020275162"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Then we score the predicted output from model on our test data against our ground truth test data.\n",
    "y_predict = modelt.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(Y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an accuracy score of 90%, which is significantly better than SVM but less than Random Forest classification.\n",
    "Let’s also take a look at our confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>779</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>55</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                 779                  76\n",
       "True Positive                  55                 471"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(Y_test, y_predict),\n",
    "    columns=['Predicted Negative', 'Predicted Positive'],\n",
    "    index=['True Negative', 'True Positive']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "Spam is becoming a serious threat for regular users of emails, businesses and corporate firms. In this assignment, we have applied three different algorithms(Random Forest, SVM and decision tree) on the set of data  which consist of spam and ham mails collected from our sources. Result analysis on test data shows that Ramdom Forest outperforms SVM and decision tree classifiers when it comes to accuracy.  \n",
    "\n",
    "Random Forest --> 94%\n",
    "\n",
    "Support Vector Machine --> 83%\n",
    "\n",
    "Decision Tree--> 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
